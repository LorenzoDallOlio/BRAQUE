{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4b0050-2e03-4e68-a405-f87bd0189851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BRAQUE - Bayesian Reduction for Amplified Quantisation in Umap Embedding\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import collections\n",
    "\n",
    "import umap\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# if you have an Intel processor, we strongly suggest you to install sklearnex\n",
    "# and uncomment the following two lines\n",
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn()\n",
    "\n",
    "\n",
    "# SETUP ------------------------------------------------------------------------\n",
    "\n",
    "# fix some global parameters\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "old_fontsize = plt.rcParams['font.size']\n",
    "new_fontsize = 15  # font size to be used in all plots where not specified, we suggest to use larger values than default as most plots contain multiple labels and texts that are helpful for the results interpretation.\n",
    "plt.rcParams.update({'font.size': 15})\n",
    "\n",
    "# select where to redirect standard output, otherwise just comment these two lines\n",
    "output_file = \"output.txt\"\n",
    "sys.stdout = open(output_file,\"w\", 1)\n",
    "\n",
    "# select if you wish a verbose print or not\n",
    "verbose_output = True\n",
    "verboseprint = print if verbose_output else lambda *a, **k: None\n",
    "\n",
    "\n",
    "# dataset\n",
    "area = \"PATH/TO/DATA\"\n",
    "\n",
    "# reference file\n",
    "reference_file = \"reference.csv\"  # path to reference .csv file\n",
    "correspondence_column = 'Ab'  # header of the reference file column which contains features names, may contain variants\n",
    "naming_column = 'Name'        # header of the reference file column which contains features names that shall be used in plots/results\n",
    "interpretative_column = 'Significance'  # header of the reference file column which contains features associated propery\n",
    "importance_column = 'Lineage Defining'  # header of the reference file column which contains 1 for important features that should be used for summary plot\n",
    "\n",
    "markers_subgroup = ''    # header of the reference file column which might opionally be used to keep only a subset of features. if used shall be a 0/1 coded column, with 1 for keeping the feature at that specific row, or 0 to exlude it. use an empty string ('') to avoid such subselection\n",
    "\n",
    "\n",
    "# IMPORTANT PARAMETERS               Suggested val\n",
    "perform_features_selection = True    # True\n",
    "\n",
    "# BayesianGaussianMixture\n",
    "perform_lognormal_shrinkage = True   # wether or not perform LNS (Lognormal Shrinkage preprocessing from Dall'Olio et al. https://doi.org/10.3390/e25020354\n",
    "max_n_gaussians = 15                 # 15 for cores and 20 for limphnode\n",
    "contraction_factor = 5.              # between 2 and 10, suggested 5, is affected by the chosen base for the logarithm\n",
    "\n",
    "# UMAP\n",
    "nn = 50                              # 50, but should not brutally affect varying in hte range 30~100\n",
    "metric = 'euclidean'                 # euclidean if no \n",
    "\n",
    "# load premade steps if needed\n",
    "load_embed = False\n",
    "load_db = False\n",
    "load_clusters = False\n",
    "large_db_procedure = False\n",
    "if large_db_procedure:\n",
    "    load_embed = True\n",
    "    load_db = True\n",
    "    load_clusters = False\n",
    "\n",
    "\n",
    "# saving directories for resulting plots\n",
    "base_folder = \"./\"\n",
    "folder = base_folder+\"results/\"+area+markers_subgroup+\"/\"\n",
    "embedding_storage = base_folder+\"embeddings/\"+area+markers_subgroup\n",
    "\n",
    "\n",
    "# start of the analysis\n",
    "verboseprint(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e68b077-82e5-413b-be9c-8f70fbaef592",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FUNCTIONS DEFINITIONS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# UTILS ---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def custom_colormap_for_many_clusters(n_clusters=None, random_seed=42, \n",
    "                                      bright_threshold=0.2):\n",
    "    \n",
    "    \"\"\"New colormap to deal properly with 20+ clusters scenarios.\n",
    "    \n",
    "    \\nn_clusters (integer): number of clusters, each of which will correspond to a color in the resulting output\n",
    "    \\nrandom_seed (integer): random seed for color order, different seeds will give different color orders\n",
    "    \\nbright_threshold (float, between 0 an 1): value used to discard shades of white and very bright colors, the higher the less colors will be used for the colormap\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # =============================================================================\n",
    "    # New colormap to deal properly even 20+ clusters scenarios\n",
    "    # =============================================================================\n",
    "    \n",
    "    colors_rgb_list = [(0.0, 0.0, 0.0)]  # start with black for noise\n",
    "    colors_list = mcolors.CSS4_COLORS\n",
    "    output_colors = list(colors_list.keys())  # use all colors\n",
    "    if n_clusters is None:\n",
    "        n_clusters = len(output_colors)  # 17, 120\n",
    "\n",
    "    # shuffle colors to avoid similar colors always adjacent on list\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(output_colors)\n",
    "    \n",
    "    # remove bright colors (e.g. shades of white) and black (since we put it by hand in 1st position for noise\n",
    "    for i in output_colors:\n",
    "        color_rgb = 1-np.asarray(mcolors.hex2color(colors_list[i]))\n",
    "        if np.sum(color_rgb**2)**0.5 < bright_threshold or color_rgb.min()==1.:  # removing also black since we put it by hand in 1st position for noise\n",
    "            output_colors.remove(i)\n",
    "            verboseprint(\"removed color\", i)\n",
    "\n",
    "    # keep only n_clusters different colors in the final colormap\n",
    "    for i in output_colors[:n_clusters]:\n",
    "        colors_rgb_list.append(mcolors.hex2color(colors_list[i]))\n",
    "\n",
    "    return mcolors.ListedColormap(colors_rgb_list)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def find_names(markers_names):\n",
    "    \n",
    "    \"\"\"Substitutes columns names with pre-defined standard names contained in reference file\n",
    "    \n",
    "    \\nmarkers_names (string or list/array-like): either signle string to convert or list of strings to convert to standard name.\n",
    "    It is important for these values to exactly correspond to a value of the reference file corresponding column\n",
    "    \n",
    "    NOTE: we assume that the global variables: reference, correpsondence_column, and naming_column are properly defined\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    official_markers_names = []\n",
    "    \n",
    "    # <specific for the paper>\n",
    "    if area == \"DatasetCodex\":\n",
    "        if isinstance(markers_names, str):\n",
    "            return [markers_names]\n",
    "        else:\n",
    "            return markers_names\n",
    "    \n",
    "    elif isinstance(markers_names, str):  \n",
    "        # only 1 iteration to perform, otherwise we cicle over the single characters within the string\n",
    "        try:\n",
    "            ab = [a for a in reference.loc[:, correspondence_column] if markers_names in a][0]\n",
    "            official_markers_names.append(reference[reference.loc[:, correspondence_column] == ab].loc[:, naming_column].iloc[0]) \n",
    "        except IndexError:\n",
    "            official_markers_names.append(markers_names)\n",
    "        return np.asarray(official_markers_names)\n",
    "    \n",
    "    else:\n",
    "        for mn in markers_names:\n",
    "            try:\n",
    "                ab = [a for a in reference.loc[:, correspondence_column] if mn in a][0]\n",
    "                official_markers_names.append(reference[reference.loc[:, correspondence_column] == ab].loc[:, naming_column].iloc[0]) \n",
    "            except IndexError:\n",
    "                official_markers_names.append(mn)\n",
    "        return np.asarray(official_markers_names)\n",
    "\n",
    "    \n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def add_main_markers_significance_columns(db, res, find_n=3, undef_thr=0., thrs_p=0.05):\n",
    "\n",
    "    \"\"\"Add inplace to input pandas dataframe the column 'MainMarkers' and a column with their interpretation.\n",
    "    \n",
    "    \\ndb (pandas.DataFrame): processed dataframe containing cells x markers values\n",
    "    \\nres (pandas DataFrame with shape n_clusters x n_features): DataFrame containing effect_sizes resulting from robust d computation by Vendekar et al. of how much each feature is overexpressed for each cluster, this object is the first output of the function 'markers_importance_per_cluster'\n",
    "    \\nfind_n (positive integer): how many main markers to find, at most\n",
    "    \\nundef_thr (non-negative float): threshold below which an effect size is never considered relevant\n",
    "    \\nthrs_p (float in range ]0, 1[): which significance threshold should be adopted for p_values.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    prevalent_population = []\n",
    "    markers = []\n",
    "    db[interpretative_column] =  \"\"\n",
    "    db['MainMarkers'] = \"\"\n",
    "    unique_clusters = np.unique(db.loc[:, 'cluster'])\n",
    "\n",
    "    # using threshold suggested in robust Cohen's d by Vandekar et Al (2020)\n",
    "    # undef_thr = 0.1\n",
    "\n",
    "    for _ in range(1, find_n+1):\n",
    "        # find markers names for 'find_n' markers with maximum effect size\n",
    "        maxmarkers = [mrkr.split(sep='_')[-1] for mrkr in res.iloc[:, :].apply(lambda row: row.nlargest(_).index[-1], axis=1)]\n",
    "        verboseprint(\"\\n\\n------------------------------------------\\n\",\n",
    "                     _, \"most expressed marker per cluster:\\n\")\n",
    "        \n",
    "        for i in unique_clusters:\n",
    "            # if marker has larger effect size than 'udenf_thr' it will appear in MinMarkers column, \n",
    "            # otherwise it will be considered not expressed enough and therefore ignored\n",
    "            if np.max(res.loc[i]) > undef_thr:\n",
    "                # add the effect size value in the label\n",
    "                level = res.loc[i, res.iloc[:, :].apply(lambda row: row.nlargest(_).index[-1], axis=1)[i]]\n",
    "                if level < undef_thr:\n",
    "                    continue  # check, this could give error if no significant marker is found\n",
    "                              # and maxmarker/prevalent population result in being shorter\n",
    "\n",
    "                maxmarker = maxmarkers[i-min(unique_clusters)]\n",
    "\n",
    "                \n",
    "                try:\n",
    "                    ab = [a for a in reference.loc[:, correspondence_column] if maxmarker in a][0]\n",
    "                    pop = reference[reference.loc[:, correspondence_column] == ab].loc[:, interpretative_column].iloc[0]\n",
    "                    markers.append(find_names(maxmarker)[0])  # using Name instead of Ab\n",
    "                    \n",
    "                    # label markers with no clear correspondence as unclear\n",
    "                    if pd.isna(pop):\n",
    "                        prevalent_population.append(\"unclear\")\n",
    "                        verboseprint(\"cluster\", i, \"unclear nan\", maxmarker)\n",
    "                    else:\n",
    "                        prevalent_population.append(pop)\n",
    "                        verboseprint(\"cluster\", i, maxmarker, pop)\n",
    "                except IndexError:\n",
    "                    prevalent_population.append(\"unclear\")\n",
    "                    verboseprint(\"cluster\", i, \"unclear\", maxmarker)\n",
    "\n",
    "                # for larger values of 'find_n' insert some newlines, in this case every 6 markers names\n",
    "                if find_n > 6:\n",
    "                    markers[-1] += ':'+str(level)[:3]\n",
    "                    if _%7 == 6:\n",
    "                        markers[-1] += '\\n'\n",
    "            else:\n",
    "                prevalent_population.append(\"unclear\")\n",
    "                verboseprint(\"cluster\", i, \"undefined\")\n",
    "\n",
    "            # write MainMarkers column and interpretation column\n",
    "            if markers != []:\n",
    "                mask = db[db.loc[:, \"cluster\"]==i].index.values\n",
    "                db.loc[mask, interpretative_column] += \" | \"+prevalent_population[-1]\n",
    "                db.loc[mask, \"MainMarkers\"] += \" | \"+markers[-1]\n",
    "            else:\n",
    "                mask = db[db.loc[:, \"cluster\"]==i].index.values\n",
    "                db.loc[mask, interpretative_column] += \" | \"+prevalent_population[-1]\n",
    "                db.loc[mask, \"MainMarkers\"] += \" | \"\n",
    "\n",
    "\n",
    "    # nullify noise labels\n",
    "    mask = db.loc[:, 'cluster'] == -1  # do not plot noise (a.k.a. removed cells)\n",
    "    db.loc[:, \"MainMarkers\"][mask] = ''\n",
    "    db.loc[:, \"Significance\"][mask] = ''\n",
    "\n",
    "    verboseprint(\"\\n\\n\")\n",
    "    for i in np.unique(db.loc[:, \"cluster\"]):\n",
    "        verboseprint(i, db.loc[:, \"MainMarkers\"][db.loc[:, \"cluster\"]==i].iloc[0])\n",
    "\n",
    "    return db\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PIPELINE -------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def features_selection(db, reference, drop_unclear=True, \n",
    "                       drop_missing=True, to_drop = ['IGNORE'], \n",
    "                       special_keeps=[]):\n",
    "    \n",
    "    \"\"\"Perform features selection over a dataframe, given a reference file on which column to keep/discard.        \n",
    "    \n",
    "    \\ndb (pandas.DataFrame): dataframe containing cells x markers values, on which the features selection is going to be performed\n",
    "    \\nreference (pandas.DataFrame): dataframe which maps db columns to different properties for each column, mandatory properties are Ab and Significance\n",
    "    \\ndrop_unclear (boolean): whether to drop markers with no corrispondence in the reference file\n",
    "    \\ndrop_missing (boolean): whether to drop markers with missing significance\n",
    "    \\nto_drop (list or array-like): drop markers whose column named 'significance' value, in the reference file, is part of this list\n",
    "    \\nspecial_keeps (list or array-like): markers whose name is in this lisy will be kept anyway if they have at least a significance and it's != IGNORE\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    for column in db.columns.values:\n",
    "        marker = column.split(sep='_')[-1]\n",
    "        try:\n",
    "            # <SPECIFIC FOR THE PAPER>\n",
    "            if marker == 'lambda':\n",
    "                antibody = [ab for ab in reference.loc[:, correspondence_column] if marker in ab][1]  # since kappasinelambda comes at 0\n",
    "            else:\n",
    "                antibody = [ab for ab in reference.loc[:, correspondence_column] if marker in ab][0]\n",
    "\n",
    "            if markers_subgroup:\n",
    "                # discard markers not in the specific subgroup\n",
    "                to_discard = reference[reference.loc[:, correspondence_column] == antibody].loc[: , markers_subgroup].iloc[0] == 0\n",
    "                if to_discard:\n",
    "                    db.drop(columns=column, inplace=True)\n",
    "                    verboseprint(\"Dropping\", column, \"since this marker is not in\", markers_subgroup, 'subgroup.')\n",
    "                    continue\n",
    "\n",
    "            # drop unwanted populations, and/or missing/unclear ones\n",
    "            population = reference[reference.loc[:, correspondence_column] == antibody].loc[:, interpretative_column].iloc[0]\n",
    "\n",
    "            # missing\n",
    "            if drop_missing and pd.isna(population):\n",
    "                db.drop(columns=column, inplace=True)\n",
    "                verboseprint(\"Dropping\", column, \"due to 'no specific significance'.\")\n",
    "\n",
    "            # unwanted\n",
    "            else:\n",
    "                for drop_candidate in to_drop:\n",
    "                    if drop_candidate in population:\n",
    "                        db.drop(columns=column, inplace=True)\n",
    "                        verboseprint(\"Dropping\", column, \"due to unwanted significance:\", drop_candidate)\n",
    "\n",
    "        # unclear\n",
    "        except IndexError:\n",
    "            if drop_unclear:\n",
    "                db.drop(columns=column, inplace=True)\n",
    "                verboseprint(\"Dropping\", column, \"due to 'unknown significance'.\")\n",
    "                \n",
    "        n_markers = len(db.columns)\n",
    "\n",
    "    return db\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def lognormal_shrinkage(db, subsampling=1, max_n_gaussians=20, \n",
    "                        contraction_factor=5., populations_plot=False):\n",
    "    \n",
    "    \"\"\"Perform Lognormal Shrinkage preprocessing over a pandas datafame\n",
    "    \n",
    "    \\ndb (pandas.DataFrame): dataframe containing cells x markers values, on which the lognormal shrinkage preprocessing is going to be performed\n",
    "    \\nsubsampling (positive integer, between 1 and len(db)): subsampling parameter, take 1 cell every N. in order to speed up gaussian mixture fitting procedure\n",
    "    \\nmax_n_gaussians (positive integer, >=2): maximum number of fittable lognormal distributions for a single marker, keep in mind that the higher the slower and more precise the algorithm. To tune follow guidelines from Dall'Olio et al.\n",
    "    \\ncontraction_factor (positive float, >1.): each gaussian in the log2 space is contracted by this factor to better separate candidate subpopulations. To tune follow guidelines from Dall'Olio et al.\n",
    "    \\npopulations_plot (boolean): whether or not to plot the final summary about number of candidates subpopulations for each marker, useful to tune max_n_gaussians.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.mixture import BayesianGaussianMixture\n",
    "    \n",
    "    verboseprint(\"performing Lognormal Shrinkage transformation\")\n",
    "    \n",
    "    # dropping constant feature, to avoid erros and because they are useless\n",
    "    for column in db.columns[np.where(db.var() == 0)[0]]:\n",
    "        db.drop(columns=column, inplace=True)\n",
    "        n_markers -= 1\n",
    "        verboseprint(\"Dropping\", column, \"due to Variance = 0. \\n(It is not possible to fit Bayesian Gaussian Mixture on it, the signal is constant!)\")\n",
    "\n",
    "    # robust scaling, needed to use the same mixture parameters for all the markers, which could otherwise strongly differ in range, eÃ¬outlier percentage, etc.\n",
    "    db = db/(np.abs(db - db.mean()).mean())\n",
    "\n",
    "    # add small constant to the db, in order to avoid log(0) related errors\n",
    "    epsilon = db[db>0.].min().min()\n",
    "    logdb = np.log2(db+epsilon)\n",
    "\n",
    "    # optionally print markers to check their order/number/correctness\n",
    "    verboseprint(\"list of genes to process\")\n",
    "    for i, column in enumerate(db.columns):\n",
    "        verboseprint(i+1, column)\n",
    "        \n",
    "    with open(folder+\"/preprocessed_markers.txt\", 'w') as f:\n",
    "        print(\"list of features which completed the lognormal shrinkage preprocessing step\", file=f)\n",
    "\n",
    "    n_populations = {}  # to store number of gaussians per marker information\n",
    "    for i, column in enumerate(db.columns):\n",
    "        verboseprint(i+1, column)\n",
    "\n",
    "        use = logdb.loc[:, column]\n",
    "        # Fitting a Bayesian Gaussian Mixture, with maximum number of gaussians = max_n_gaussian, over each marker separately\n",
    "        gm = BayesianGaussianMixture(n_components=max_n_gaussians, max_iter=20000, n_init=1,\n",
    "                                     verbose=51, verbose_interval=100, tol=1e-2,\n",
    "                                     random_state=42, covariance_type='full')\n",
    "        gm.fit(np.expand_dims(use.T[::subsampling], 1))  # gaussian mixture fit using N=floor(1/subsampling) cells\n",
    "        preds = gm.predict(np.expand_dims(use.T, 1))  # gaussian mixture prediction for every cell\n",
    "\n",
    "        # Shrinkage step: shrinking each point towards its belonging gaussian's mean\n",
    "        preds_final = gm.means_[preds].T[0].T - (gm.means_[preds].T[0].T - use.T)/contraction_factor\n",
    "\n",
    "        # backtransforming shrinked data to the original space, and setting each markers minimum equal to 0\n",
    "        db.loc[:, column] = 2**(preds_final)-epsilon\n",
    "        db.loc[:, column] -= db.loc[:, column].min()\n",
    "\n",
    "        n_populations[column] = len(np.unique(preds))\n",
    "\n",
    "        # saving checkpoint, in case something goes wrong mid-way, check for the last printed marker and perform preprocessing only on the remainin ones\n",
    "        db.to_csv(base_folder+\"quantized_dbs/\"+area+\"_quantized(\"+str(max_n_gaussians)+\",\"+str(contraction_factor)+\").csv\", \n",
    "                  index=False)\n",
    "\n",
    "        # print to file preprocessed markers list\n",
    "        with open(folder+\"/preprocessed_markers.txt\", 'a') as f:\n",
    "            print(column+\" Done, with \", len(np.unique(preds)), \"within-marker populations.\", file=f)\n",
    "\n",
    "\n",
    "    # robust standardization\n",
    "    db = (db - db.median()) / (np.abs(db - db.mean()).mean())\n",
    "\n",
    "    # save to .json file the number of gaussians (populations) per marker\n",
    "    with open(base_folder+\"quantized_dbs/\"+area+\"_n_populations.json\", \"w\") as f:\n",
    "        json.dump(n_populations, f)\n",
    "\n",
    "    if populations_plot:\n",
    "        # plot and save histogram for the number of gaussians (populations) per marker\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "        ax.hist(n_populations.values(), bins=np.arange(max_n_gaussians+1)+.5, density=False)\n",
    "        ax.set_xlabel(\"number of identified gaussian components in a marker\")\n",
    "        ax.set_ylabel(\"number of markers with a given number of gaussian components\")\n",
    "        ax.grid()\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(folder+area+\"__gaussian_components_histogram.png\", facecolor='w')\n",
    "    \n",
    "    # warn if there are saturated populations\n",
    "    n_saturated_coolumns = np.where(n_populations == max_n_gaussians, 1, 0).sum()\n",
    "    if n_saturated_coolumns:\n",
    "        import warnings\n",
    "        warnings.warn(\"Warning, some features saturated (\"\\\n",
    "                      + str(n_saturated_coolumns)\\\n",
    "                      + \" in total).\\n The algorithm could be underperforming, you could try higher 'max_n_gaussians' parameter values to avoid this scenario\",\n",
    "                      RuntimeWarning)\n",
    "    \n",
    "    return db\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def embed_dataset(db, nn=50, metric='euclidean', save_embed=True):\n",
    "\n",
    "    \"\"\"Perform the embedding on a 2D manifold of a pandas dataframe using the UMAP algorithm.\n",
    "    \n",
    "    \n",
    "    \\ndb (pandas DataFrame): the dataframe of which the embedding will be performed\n",
    "    \\nnn (integer): number of nearest neighbors to use during UMAP\n",
    "    \\nmetric (str, one of scipy-allowed distances): which metric to use during UMAP algorithm\n",
    "    \\nsave_embed: whether or not to save the resulting coordinates in the embedding space.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    verboseprint(\"Starting UMAP embedding at:\", datetime.now())\n",
    "    # NOTE: You can't use spectral initialization with fully disconnected vertices or with datasets too big\n",
    "    # Therefore, if spectral init fails, rerun using scaled pca init (scaled to 1e-4 standard deviation per dimension,\n",
    "    # as suggested by Kobak and Berens \n",
    "    try:\n",
    "        mapper = umap.UMAP(\n",
    "            n_neighbors=nn,              # 50\n",
    "            min_dist=0.0,                # 0.0\n",
    "            n_components=2,              # 2\n",
    "            random_state=42,             # 42\n",
    "            verbose=True,                # TRUE\n",
    "            low_memory=True,             # True\n",
    "            init='spectral',             # spectral\n",
    "            metric=metric,               # euclidean\n",
    "        )\n",
    "\n",
    "        embedding = mapper.fit_transform(db)\n",
    "        if save_embed:\n",
    "            embedding.dump(embedding_storage)  # save embedding coordinates for future uses\n",
    "\n",
    "    except AttributeError:\n",
    "        # scaled PCA initialization for UMAP due to huge db or other problems with spectral init\n",
    "        print(\"Encountered problems with 'spectral' init, the recommended value.\",\n",
    "              \"\\nPreparing scaled PCA initialization and repeating UMAP embedding with this instead of spectral\")\n",
    "\n",
    "        # performing PCA on as many dimension as the UMAP \n",
    "        # to use its coordinate as a starting point for UMAP's embedding\n",
    "        from sklearn.decomposition import PCA\n",
    "        pca = PCA(n_components=mapper.n_components)\n",
    "        inits = pca.fit_transform(db)\n",
    "\n",
    "        # Kobak and Berens recommend scaling the PCA results \n",
    "        # so that the standard deviation for each dimension is 1e-4\n",
    "        inits = inits / inits.std(axis=0) * 1e-4\n",
    "        verboseprint('new embedding axis std:', inits.std(axis=0))\n",
    "\n",
    "        # new trial with different init parameter\n",
    "        mapper = umap.UMAP(\n",
    "            n_neighbors=nn,              # 50\n",
    "            min_dist=0.0,                # 0.0\n",
    "            n_components=2,              # 2\n",
    "            random_state=42,             # 42\n",
    "            verbose=True,                # True\n",
    "            low_memory=True,             # True\n",
    "            init=inits,                  # inits\n",
    "            metric=metric,               # euclidean\n",
    "        )\n",
    "\n",
    "        embedding = mapper.fit_transform(db)    \n",
    "        if save_embed:\n",
    "            embedding.dump(embedding_storage)  # save embedding coordinates for future uses\n",
    "\n",
    "    verboseprint(\"Finished UMAP embedding at:\", datetime.now())\n",
    "    return embedding\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "def markers_importance_per_cluster(use, clusters, compare_with='rest'):\n",
    "    \n",
    "    \"\"\"Compute markers importance within each cluster\n",
    "    \n",
    "    \\nuse (pandas DataFrame): the dataframe to use for effect size and p-value computations, either original robustly scaled (suggested) or preprocessed db\n",
    "    \\nclusters (list or array-like): sequence of clusters labels (which cluster each point belongs to)\n",
    "    \\ncompare_with (str, either 'rest' or 'all'): whether or not to compare each cluster with the rest of the cells or with all the cells (including the cluster itself), the second option could be used to have a common reference for effect sizes and compare different markers effect sizes, but qould make the two samples for the ttest not indipendent.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    unique_clusters = np.unique(clusters)\n",
    "\n",
    "\n",
    "    # numerosity, mean, and std dev for each cluster\n",
    "    n1 = use.groupby(clusters).count()\n",
    "    m1 = use.groupby(clusters).mean()\n",
    "    s1 = use.groupby(clusters).std()\n",
    "\n",
    "    # numerosity, mean, and std dev for the whole sample\n",
    "    n = len(use)\n",
    "    m = use.mean()\n",
    "    s = use.std()\n",
    "\n",
    "    # numerosity, mean, and std dev for the rest of cells for each cluster\n",
    "    n2 = n-n1\n",
    "    m2 = (m*n-m1*n1)/n2\n",
    "    s2 = ((n*s**2 - n1*(s1**2+(m1-m)**2))/n2 - (m2-m)**2 ) ** .5\n",
    "\n",
    "    # output DataFrames where effect size and p-values will be stored\n",
    "    res = m1.copy()    # effect size\n",
    "    res_p = m1.copy()  # p-values\n",
    "\n",
    "\n",
    "\n",
    "    if compare_with.lower() == 'all':\n",
    "        for column in m1.columns:\n",
    "            for ind in m1.index:\n",
    "                # fast and vectorized version of t-test for p-values\n",
    "                res_p.loc[ind, column] = st.ttest_ind_from_stats(m1.loc[ind, column],\n",
    "                                                              s1.loc[ind, column],\n",
    "                                                              n1.loc[ind, column],\n",
    "                                                              m.loc[column],\n",
    "                                                              s.loc[column],\n",
    "                                                              n,\n",
    "                                                              equal_var=False,\n",
    "                                                              alternative='greater')[-1]\n",
    "        # effect size computed with robust d by Vendekar et al.\n",
    "        res = (-1)**(m1<m) * np.sqrt( (m1-m)**2 / ((n1+n)/n1*s1**2 + (n1+n)/n*s**2) )\n",
    "\n",
    "    elif compare_with.lower() == 'rest':\n",
    "        for column in m1.columns:\n",
    "            for ind in m1.index:\n",
    "                # fast and vectorized version of t-test for p-values\n",
    "                res_p.loc[ind, column] = st.ttest_ind_from_stats(m1.loc[ind, column],\n",
    "                                                              s1.loc[ind, column],\n",
    "                                                              n1.loc[ind, column],\n",
    "                                                              m2.loc[ind, column],\n",
    "                                                              s2.loc[ind, column],\n",
    "                                                              n2.loc[ind, column],\n",
    "                                                              equal_var=False,\n",
    "                                                              alternative='greater')[-1]\n",
    "        # effect size computed with robust d by Vendekar et al.\n",
    "        res = (-1)**(m2>m1) * np.sqrt( (m1-m2)**2 / ((n1+n2)/n1*s1**2 + (n1+n2)/n2*s2**2) )\n",
    "\n",
    "    else:\n",
    "        verboseprint(\"\"\"Warning, it was unclear the data portion you wanted to compare results\n",
    "              against, please use 'all' for using all the sample or 'rest' for using\n",
    "              every cell outside the considered cluster.\"\"\")\n",
    "\n",
    "    return res, res_p\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def most_important_effect_sizes(rs, rs_p, n, thrs_p=0.05, \n",
    "                                save_plot=False, path=base_folder+\"cluster__expression.png\"):\n",
    "    \n",
    "    \"\"\"Horizontal bar plot with highest effect size markers for a given cluster\n",
    "    \n",
    "    \\nrs (numpy array): a row for the 'res' output by 'markers_importance_per_cluster', represents each marker's effect size for a given cluster\n",
    "    \\nrs_p (numpy array): a row for the 'res_p' output by 'markers_importance_per_cluster', represents each marker's p-value for a given cluster\n",
    "    \\nn (integer): how many markers to plot, in descending order of effect size\n",
    "    \\nthrs_p (float between 0 and 1): change color for the horizontal bars based on whether or not the p-value is below this significance threshold\n",
    "    \\nsave (boolean): whether or not to save the resulting plot\n",
    "    \\npath (str): where to store the resulting plot (if saved)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # markers order based on descending effect size for top n markers                        \n",
    "    order = np.argsort(rs)[::-1][:n]\n",
    "    # consecutive differences\n",
    "    diffs = np.ediff1d(rs[order])\n",
    "    \n",
    "    # 'last' is the last candidate to be in Tier 1 markers (within 0 and gap1)\n",
    "    try:\n",
    "        # 'last' = last positive effect size marker or second to last marker for top n effect sizes\n",
    "        last = np.where(rs[order][:-2]<0.)[0][0]\n",
    "        if last == 0:\n",
    "            last = n-2\n",
    "    except IndexError:\n",
    "        last = n-2\n",
    "\n",
    "    gap1 = diffs[:last].argmin()+1             # defining most expressed markers\n",
    "    gap2 = diffs[gap1:last+1].argmin()+gap1+1  # defining possibly expressed markers\n",
    "\n",
    "    \n",
    "    # creating most expressed marker subtitle label\n",
    "    lbl = \"\\nTier 1:\"\n",
    "    for _ in np.arange(gap1):\n",
    "        lbl += \" | \"+find_names(db.columns[order][_])[0]+\" \"+str(rs[order][_])[:4]\n",
    "    lbl += \"\\nTier 2:\"\n",
    "    for _ in np.arange(gap1, gap2):\n",
    "        lbl += \" | \"+find_names(db.columns[order][_])[0]+\" \"+str(rs[order][_])[:4]\n",
    "\n",
    "    # making effect size plot (a.k.a. most expressed markers)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    ax.barh(y=np.arange(n)[::-1],\n",
    "            width=rs[order],\n",
    "            color=np.where(rs_p[order]<thrs_p, 'b', 'r'),\n",
    "            tick_label=find_names(db.columns[order]))\n",
    "    # plot some suggested effect thresholds\n",
    "    ax.vlines(0.1,  -1, n, colors='k', linestyles='-', label='1st effect threshold <= 0.1 (None~small)')\n",
    "    ax.vlines(0.25, -1, n, colors='k', linestyles='--', label='2nd effect threshold <= 0.25 (small~medium)')\n",
    "    ax.vlines(0.4,  -1, n, colors='k', linestyles='-.', label='3rd effect threshold <= 0.4 (medium~big)')\n",
    "\n",
    "    # plot Tier 1 and Tier 2 gaps positions\n",
    "    ax.plot([-.01, max(0.4, max(rs))], [np.arange(n)[::-1][gap1]+.5, np.arange(n)[::-1][gap1]+.5],\n",
    "            'm--', label='End of tier 1 markers')\n",
    "    ax.plot([-.01, max(0.4, max(rs))], [np.arange(n)[::-1][gap2]+.5, np.arange(n)[::-1][gap2]+.5],\n",
    "            'y--', label='End of tier 2 markers')\n",
    "\n",
    "    ax.grid()\n",
    "    ax.legend(loc='lower right')\n",
    "\n",
    "    # manually adjust legend\n",
    "    handles, labels = ax.legend().axes.get_legend_handles_labels()\n",
    "    \n",
    "    handles.append(Patch(facecolor='b', edgecolor='b'))\n",
    "    labels.append(\"significant p value (<\"+str(thrs_p)+\")\")\n",
    "    handles.append(Patch(facecolor='r', edgecolor='r'))\n",
    "    labels.append(\"NOT significant p value (>\"+str(thrs_p)+\")\")\n",
    "    ax.legend(handles, labels, loc='lower right')\n",
    "\n",
    "\n",
    "    fig.suptitle(\"cluster \"+str(i)+\" composition\")\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_plot:\n",
    "        fig.savefig(path, facecolor='w')\n",
    "        \n",
    "    return lbl\n",
    "\n",
    "\n",
    "# PLOTS ----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def plot_markers_on_embedding(db, original_db, embedding, real_pos,\n",
    "                              save=False, path=folder+\"markers/\"):\n",
    "    \n",
    "    \"\"\"Summary plot for all markers.\n",
    "    \n",
    "    \\ndb (pandas.DataFrame): dataframe containing cells x markers values, of which we want to plot the summary for every marker\n",
    "    \\noriginal_db (pandas.DataFrame): original dataframe containing unprocessed markers values, of which we want to plot the summary for every marker\n",
    "    \\nembedding (2-dimensional arra-like): X,Y positions on the low dimensional embedding wherre to scatter plot data\n",
    "    \\nreal_pos (2-dimensional array-like): X,Y positions on the real space where to scatter plot data\n",
    "    \\nsave (boolean): whether or not to save the resulting plots\n",
    "    \\npath (str): path at which resulting plots should be stored, if saved\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # plot marker 'd' on the embedding and the real space, showing both its \n",
    "    # unprocessed and its processed distributions\n",
    "    for d in db.columns:\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(20, 10))\n",
    "        \n",
    "        # embedding space\n",
    "        a = ax[0][0].scatter(*embedding.T, c=db.loc[:, d], s=0.1, cmap='gist_ncar')\n",
    "        ax[0][0].grid()\n",
    "        ax[0][0].set_title('UMAP embedding')\n",
    "\n",
    "        # real space\n",
    "        ax[0][1].scatter(real_pos.iloc[:, 0], real_pos.iloc[:, 1], c=db.loc[:, d], s=0.1, cmap='gist_ncar')\n",
    "        ax[0][1].grid()\n",
    "        ax[0][1].set_title('real position')\n",
    "\n",
    "        # processed distribution\n",
    "        ax[1][0].hist(db.loc[:, d], bins=100, density=True)\n",
    "        ax[1][0].grid()\n",
    "        ax[1][0].set_title('distribution in the preprocessed log space')\n",
    "\n",
    "        # original distribution\n",
    "        ax[1][1].hist(original_db.loc[db.index, d], bins=100, density=True)\n",
    "        ax[1][1].grid()\n",
    "        ax[1][1].set_title('original raw distribution')\n",
    "\n",
    "        # colorbar and final adjustments\n",
    "        fig.colorbar(a, ax=ax[0][0]).set_label('log of marker expression', rotation=270, labelpad=15)\n",
    "        fig.suptitle('marker: '+d)\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        if save:\n",
    "            fig.savefig(path+d+\".png\", facecolor='w')\n",
    "            \n",
    "            \n",
    "\n",
    "# ---------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def plot_embedding_clusters(clusters_list, real_pos, embedding, clusterer,\n",
    "                            save_plot=False, path=base_folder+\"__clusters.png\"):\n",
    "    \n",
    "    \"\"\"Plot HDBSCAN clusters onto real space and UMAP embedding coordinates with appropriate colormap\n",
    "    \n",
    "    \\nclusters_list (list or array-like): sequence of clusters labels (which cluster each point belongs to)\n",
    "    \\nreal_pos (2-dimensional array-like): X,Y positions on the real space where to scatter plot data\n",
    "    \\nembedding (2-dimensional arra-like): X,Y positions on the low dimensional embedding wherre to scatter plot data\n",
    "    \\nclusterer (fitted hdbscan.HDBSCAN clustering object): fitted HDBSCAN clustering object used to cluster data from the embedding space\n",
    "    \\nsave_plot (boolean): whether or not to save the resulting plots\n",
    "    \\npath (str): path where to store the resulting plots if saved\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    # scatterplot each single cluster with the respective label\n",
    "    for cl in np.unique(clusters_list):\n",
    "        mask = clusters_list == cl\n",
    "        ax[0].scatter(*embedding[mask].T, s=0.1, color=custom_colormap.colors[(1+cl)%len(custom_colormap.colors)], label=str(cl))\n",
    "        ax[1].scatter(real_pos.iloc[:, 0][mask], real_pos.iloc[:, 1][mask], s=0.1, color=custom_colormap.colors[(1+cl)%len(custom_colormap.colors)])\n",
    "\n",
    "    # plot title\n",
    "    fig.suptitle(\"\\nHDBSCAN clustering (eps=\"+str(clusterer.cluster_selection_epsilon)\\\n",
    "                 +\", \"+str(100*clusterer.min_samples/len(clusters_list))[:6]+\" % of cells for min_clusters)\\nclusters: \"+str(np.max(clusters_list)+1)+\\\n",
    "                 \"   |   noise: \"+str(np.where(clusters_list == -1, 1, 0).sum()))\n",
    "\n",
    "    # show legend iff there are less than 150 total clusters, otherwise it would be unreadable\n",
    "    if len(np.unique(clusters_list)) < 150:\n",
    "        lgnd = ax[0].legend(loc='center', bbox_to_anchor=(1.15, 0.5),\n",
    "                            ncol=4, fancybox=True, shadow=True,\n",
    "                            title='clusters',\n",
    "                            fontsize=11)\n",
    "\n",
    "        # change the marker size manually for the whole legend\n",
    "        for lh in lgnd.legendHandles:\n",
    "            lh.set_sizes([200])\n",
    "\n",
    "    fig.tight_layout()\n",
    "    if save_plot:\n",
    "        fig.savefig(path, facecolor='w')\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def whole_dataset_summary_plots(db, area, alpha=0.9, size=1., legend_size=200,\n",
    "                                plot_with_legend=True, plot_without_legend=True,\n",
    "                                plot_noise=True, save_plot=False, base_path=base_folder+\"\"):\n",
    "    \n",
    "    \"\"\"Plot a Maximum of 4 plots (2 with legend and 2 without legend) which summarize the main markers and their interpretative_column for each cluster\n",
    "\n",
    "    \\ndb (pandas.DataFrame): processed dataframe containing cells x markers values\n",
    "    \\narea (str): filename of the db\n",
    "    \\nalpha (float between 0 and 1): transparency for the plots\n",
    "    \\nsize (positive float): size for the dots of the scatter plots\n",
    "    \\nlegend_size (positive float): size for the dots in the legend\n",
    "    \\nplot_with_legend (bool): whether or not to make the 2 plots with legends (may be unreadable if too many clusters/too long labels are used)\n",
    "    \\nplot_without_legend (bool): whether or not to make the 2 plots without legends (useful if with legends the plot are unreadable, please notice that this parameter is not mandatory to be opposite to 'plot_with_legend')\n",
    "    \\plot_noise (bool) whether or not to plot the noise cluster if found by HDBSCAN\n",
    "    \\nsave_plot (boolean): whether or not to save the resulting plot\n",
    "    \\npath (str): where to store the resulting plot (if saved).\n",
    "    \n",
    "    NOTE: we assume that the global variables: custom_colormap and interpretative_column are properly defined\n",
    " \n",
    "    \"\"\"\n",
    "    \n",
    "    # SUMMARY PLOTS FOR THE ADDED COLUMNS, WITH/WITHOUT LEGEND \n",
    "    for ii, show in enumerate(['MARKERS', 'MARKERS', \n",
    "                               interpretative_column.upper(), interpretative_column.upper()]):\n",
    "        if not plot_with_legend:\n",
    "            if ii%2:\n",
    "                continue\n",
    "\n",
    "        if not plot_without_legend:\n",
    "            if ii%2:\n",
    "                continue\n",
    "\n",
    "        verboseprint(\"Elaborating\", show, \"plot\")\n",
    "\n",
    "        # prepare color parameters\n",
    "        if show == 'MARKERS':\n",
    "            color = db.loc[:, \"MainMarkers\"]\n",
    "        else:\n",
    "            color = db.loc[:, interpretative_column]\n",
    "        n_clust = len(np.unique(color))\n",
    "\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(19,9))\n",
    "\n",
    "        # actual plot\n",
    "        for i, column in enumerate(np.unique(color)):\n",
    "            if column == '':\n",
    "                if plot_noise:\n",
    "                    ax.scatter(pos.iloc[:, 0][color==column], pos.iloc[:, 1][color==column],\n",
    "                               alpha=alpha, color='black',\n",
    "                               label=' Noise', s=size)\n",
    "                else:\n",
    "                    continue\n",
    "            ax.scatter(pos.iloc[:, 0][color==column], pos.iloc[:, 1][color==column],\n",
    "                       alpha=alpha, color=custom_colormap.colors[i%len(custom_colormap.colors)],\n",
    "                       label=np.unique(color)[i], s=size)\n",
    "\n",
    "        # plot legend iff ii is odd\n",
    "        if ii%2:\n",
    "            lgnd = ax.legend(loc='center left', bbox_to_anchor=(1., 0.5),\n",
    "                             ncol=1+n_clust//31, fancybox=True, shadow=True,\n",
    "                             title=show,\n",
    "                             fontsize=11)\n",
    "\n",
    "            #change the marker size manually for the whole legend\n",
    "            for lh in lgnd.legendHandles:\n",
    "                lh.set_sizes([legend_size])\n",
    "\n",
    "\n",
    "        ax.set_xlabel(\"x\")\n",
    "        ax.set_ylabel(\"y\")\n",
    "        ax.set_title(\"Real space\")\n",
    "        ax.grid()\n",
    "\n",
    "\n",
    "        fig.suptitle(\"Cell type classification - Area \"+str(area)+\" - \"+show,\n",
    "                     fontsize=20)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        if save_plot:        \n",
    "            if ii%2:\n",
    "                fig.savefig(base_path+area+\"__\"+show+\".png\", facecolor='w')\n",
    "            else:\n",
    "                fig.savefig(base_path+area+\"__\"+show+\"_nolegend.png\", facecolor='w')\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "def plot_cluster_spatial_location(lbl, embedding, pos, mask, \n",
    "                                  save_plot=False, path=base_folder+\"cluster__position.png\"):\n",
    "\n",
    "    \"\"\"Plot cluster position in UMAP embedding and in real space, together with a summary of Tier1 and Tier2 markers.\n",
    "    \n",
    "    \\nlbl (str): string produced by func containing Tier 1 and Tier 2 markers, which will be used as header for the plot \n",
    "    \\nembedding (2-dimensional arra-like): X,Y positions on the low dimensional embedding wherre to scatter plot data\n",
    "    \\npos (2-dimensional array-like): X,Y positions on the real space where to scatter plot data    \n",
    "    \\nmask (array-like of boolean values): boolean values corresponding to whether or not a single cell is part of the considered cluster\n",
    "    \\nsave_plot (boolean): whether or not to save the resulting plot\n",
    "    \\npath (str): where to store the resulting plot (if saved)\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "    # baseline light grey plot for whole sample\n",
    "    ax[0].scatter(embedding.T[0], embedding.T[1], s=0.1, color='lightgrey', label='other')\n",
    "    ax[1].scatter(pos.iloc[:, 0], pos.iloc[:, 1], s=0.1, color='lightgrey')\n",
    "\n",
    "    # cluster plot with vivid red\n",
    "    ax[0].scatter(embedding.T[0][mask], embedding.T[1][mask], s=3, color='r', label=str(i))\n",
    "    ax[1].scatter(pos.iloc[:, 0][mask], pos.iloc[:, 1][mask], s=3, color='r')\n",
    "\n",
    "    ax[0].set_title(\"UMAP embedding\")\n",
    "    ax[1].set_title(\"Real position\")\n",
    "\n",
    "\n",
    "    fig.suptitle(\"Cluster \"+str(i)+\": containing \"+str(np.asarray(mask).sum())+\" cells. Reporting marker name and effect size\"+lbl)\n",
    "\n",
    "    lgnd = ax[0].legend(loc='center', bbox_to_anchor=(1., 0.5),\n",
    "                        ncol=1, fancybox=True, shadow=True,\n",
    "                        title='clusters',\n",
    "                        fontsize=11)\n",
    "\n",
    "    #change the marker size manually for the whole legend\n",
    "    for lh in lgnd.legendHandles:\n",
    "        lh.set_sizes([200])\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_plot:\n",
    "        fig.savefig(path, facecolor='w')\n",
    "    \n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def monitoring_features_plot(original_db, mask, cluster_name, \n",
    "                             monitoring_features, palette=[(0.2, 0.5, 1.), (1., 0.2, 0.2)], \n",
    "                             save_plot=False, path=base_folder+\"cluster__monitoring_featuresining.png\"):\n",
    "    \n",
    "    \"\"\"Plot Kernel Density Estimation to compare a cluster with whole dataset using some monitoring features.\n",
    "    \n",
    "    \\noriginal_db (pandas.DataFrame): original and unprocessed dataframe containing cells x markers values\n",
    "    \\nmask (array-like of boolean values): boolean values corresponding to whether or not a single cell is part of the considered cluster\n",
    "    \\ncluster_name (str): label to use for the cluster name in the plot \n",
    "    \\nmonitoring_features (list or array-like): list of features over which each cluster will be compared with whole sample using KDE plots\n",
    "    \\npalette (list of 2 rgb tuples): color palette to use for cluster or whole sample cells and KDE plots \n",
    "    \\nsave_plot (boolean): whether or not to save the resulting plot\n",
    "    \\npath (str): where to store the resulting plot (if saved)\n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    # build 'sub_db' for monitoring features plot, such pandas DataFrame will be composed\n",
    "    # of only the monitoring features, and will contain an extra column based on which \n",
    "    # cells belong to a cluster or to the whole sample (cluster cells will have duplicates)\n",
    "    sub_db = pd.DataFrame({'expression': original_db.loc[:, monitoring_features].T.stack().values,\n",
    "                           'marker': [a[0] for a in original_db.loc[:, monitoring_features].T.stack().index],\n",
    "                           'cluster': list(np.where(mask, \n",
    "                                                    cluster_name,\n",
    "                                                    'whole sample'))*len(monitoring_features)})\n",
    "    sub_db2 = sub_db[sub_db.loc[:, 'cluster']!='whole sample']\n",
    "    sub_db2.loc[:,'cluster']='whole sample'\n",
    "    sub_db = pd.concat([sub_db,sub_db2])\n",
    "    \n",
    "    plt.figure(figsize=(18, 8))\n",
    "\n",
    "    # # in the sns.FacetGrid class, the 'hue' argument is the one that is the one that will be represented by colors with 'palette'\n",
    "    g = sns.FacetGrid(data=sub_db, row='marker', hue='cluster', palette=palette,\n",
    "                      sharey=False, legend_out=True, height=1, aspect=2*len(monitoring_features)-2)\n",
    "\n",
    "    # then we add the densities kdeplots for each month\n",
    "    g.map(sns.kdeplot, 'expression',\n",
    "          bw_adjust=1, clip_on=False,\n",
    "          fill=True, alpha=0.4, linewidth=2)\n",
    "\n",
    "    # here we add a horizontal line for each plot\n",
    "    g.map(plt.axhline, y=0, lw=5, clip_on=False)\n",
    "\n",
    "\n",
    "    ft = 35 # bigger font size for some within plot headers\n",
    "    \n",
    "    # we loop over the FacetGrid figure axes (g.axes.flat) and add the month as text with the right color\n",
    "    # notice how ax.lines[-1].get_color() enables you to access the last line's color in each matplotlib.Axes\n",
    "    for ii, axis in enumerate(g.axes.flat):\n",
    "\n",
    "        # <Specific for the paper>\n",
    "        if area == 'DatasetCodex':\n",
    "            axis.text(-1000, 0.0, monitoring_features[ii],\n",
    "                      fontweight='bold', fontsize=ft, color=axis.lines[0].get_color())\n",
    "            axis.text(8000, 0.0, 'effect size: '+str(res.loc[i, monitoring_features[ii]])[:6],\n",
    "                      fontweight='bold', fontsize=ft, color=axis.lines[0].get_color())\n",
    "        else:\n",
    "            axis.text(-13, 0.0, monitoring_features[ii],\n",
    "                      fontweight='bold', fontsize=ft, color=axis.lines[0].get_color())\n",
    "            axis.text(100, 0.0, 'effect size: '+str(res.loc[i, monitoring_features[ii]])[:6],\n",
    "                      fontweight='bold', fontsize=ft, color=axis.lines[0].get_color())\n",
    "        axis.plot([sub_db2.groupby('marker').median().loc[monitoring_features[ii]],\n",
    "                   sub_db2.groupby('marker').median().loc[monitoring_features[ii]]],\n",
    "                   axis.get_ylim(), 'r--', linewidth=3)\n",
    "        axis.plot([sub_db.groupby('marker').median().loc[monitoring_features[ii]],\n",
    "                   sub_db.groupby('marker').median().loc[monitoring_features[ii]]], axis.get_ylim(), 'b--', linewidth=3)\n",
    "        axis.set_ylabel(\"\")\n",
    "\n",
    "    # we use matplotlib.Figure.subplots_adjust() function to get the subplots to overlap\n",
    "    g.fig.subplots_adjust(hspace=-0.)\n",
    "\n",
    "    # eventually we remove axes titles, yticks and spines\n",
    "    g.set_titles(\"\")\n",
    "    g.set(yticks=[])\n",
    "    g.despine(bottom=True, left=True)\n",
    "\n",
    "    plt.setp(axis.get_xticklabels(), fontsize=ft, fontweight='bold')\n",
    "    plt.xlabel('Marker Expression (a.u.)', fontweight='bold', fontsize=ft)\n",
    "    g.fig.suptitle(\"\", ha='right', fontsize=ft+5, fontweight=20)\n",
    "    if area == 'DatasetCodex':\n",
    "        plt.xlim(-1000, 10000)\n",
    "    else:\n",
    "        plt.xlim(-13, 150)\n",
    "    plt.xticks(fontweight='bold', fontsize=ft//2)\n",
    "    plt.legend()\n",
    "    \n",
    "    # manually adjust the legend labels\n",
    "    handles, labels = [], []\n",
    "    handles.append(Patch(facecolor='b', edgecolor='b', alpha=0.4))\n",
    "    labels.append(\"whole sample\")    \n",
    "    handles.append(Patch(facecolor='r', edgecolor='r', alpha=0.4))\n",
    "    labels.append(cluster_name)\n",
    "    handles.append(Line2D([0], [0], linestyle='--', linewidth=5, color='b'))\n",
    "    labels.append(\"marker median (whole sample)\")\n",
    "    handles.append(Line2D([0], [0], linestyle='--', linewidth=5, color='r'))\n",
    "    labels.append(\"marker median (cluster)\")\n",
    "    \n",
    "    plt.legend(handles, labels, ncol=2, loc='upper center',\n",
    "               bbox_to_anchor=(0.5, len(monitoring_features)+1.01), fontsize=ft)\n",
    "    \n",
    "    \n",
    "    if save_plot:\n",
    "        plt.savefig(path, facecolor='w')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c564fc7-b9ad-47b7-97dc-95b41e49d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_colormap = custom_colormap_for_many_clusters()\n",
    "\n",
    "\n",
    "if not large_db_procedure:\n",
    "    # FEATURES SELECTION, PREPROCESSING, and UMAP embedding\n",
    "\n",
    "    # load reference file for columns: define subgroups, discarded markers, etc.\n",
    "    reference = pd.read_csv(reference_file)\n",
    "    print(\"loaded reference file\")\n",
    "\n",
    "\n",
    "    # creating directories to store plots/embedding\n",
    "    for directory in [folder, folder+\"positions/\", folder+'expressions/', \n",
    "                      folder+\"markers/\", base_folder+\"data\", \n",
    "                      base_folder+\"quantized_dbs\", base_folder+\"embeddings\"]:\n",
    "        if not os.path.isdir(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(\"creating\", directory)\n",
    "        else:\n",
    "            verboseprint(\"using pre existing\", directory)\n",
    "\n",
    "    # eventually load, previously preprocessed db\n",
    "    if load_db:\n",
    "        db = pd.read_csv(base_folder+\"quantized_dbs/\"+area+\"_quantized(\"+str(max_n_gaussians)+\",\"+str(contraction_factor)+\").csv\")\n",
    "\n",
    "        # robust standardization\n",
    "        db = (db-db.median())/(np.abs(db - db.mean()).mean())\n",
    "\n",
    "        # select a specific marker subgroup, if required\n",
    "        if markers_subgroup:\n",
    "            for column in db.columns.values:\n",
    "                verboseprint(column)\n",
    "                marker = column.split(sep='_')[-1]\n",
    "\n",
    "                # <SPECIFIC FOR THE PAPER>\n",
    "                if marker == 'lambda':\n",
    "                    ab = [a for a in reference.loc[:, correspondence_column] if marker in a][1]  # since kappasinelambda comes at 0\n",
    "                else:\n",
    "                    ab = [a for a in reference.loc[:, correspondence_column] if marker in a][0]\n",
    "\n",
    "                # discard markers not in the specific subgroup\n",
    "                to_discard = reference[reference.loc[:, correspondence_column] == ab].loc[: , markers_subgroup].iloc[0] == 0\n",
    "                if to_discard:\n",
    "                    db.drop(columns=column, inplace=True)\n",
    "                    verboseprint(\"Dropping\", column, \"since this marker is not in\", markers_subgroup, 'subgroup.')\n",
    "\n",
    "        # measure the number of the remaining markers\n",
    "        n_markers = len(db.columns)\n",
    "\n",
    "\n",
    "    else:\n",
    "        # <SPECIFIC FOR THE PAPER>\n",
    "        if area == \"DatasetCodex\":\n",
    "            verboseprint(\"Starting load of the data at:\", datetime.now())\n",
    "            original_db = pd.read_csv(base_folder+\"data/\"+area+\".csv\").iloc[:, 1:]\n",
    "\n",
    "            # to have marker names in the same format as we are used, which is just marker name with no channel or separators\n",
    "            new_names = {key:value for (key,value) in zip(original_db.columns, [a.split('.')[-1] for a in original_db.columns])}\n",
    "            original_db.rename(columns=new_names, inplace=True)  # renames columns to just marker name\n",
    "\n",
    "            # separate into db for markers and pos for spatial position\n",
    "            db = original_db.iloc[:, 4:-3]\n",
    "            pos = original_db.iloc[:, :2]\n",
    "            print(\"data loaded\")\n",
    "\n",
    "        else:\n",
    "            verboseprint(\"Starting load of the data at:\", datetime.now())\n",
    "            original_db = pd.read_csv(base_folder+\"data/\"+area+\".csv\")\n",
    "\n",
    "            # to have marker names in the same format as we are used, which is just marker name with no channel or separators\n",
    "            new_names = {key:value for (key,value) in zip(original_db.columns, [a.split('_')[-1] for a in original_db.columns])}\n",
    "\n",
    "            # remove columns with identical names, keeping only the first one\n",
    "            if len(set(new_names.values())) != len(new_names.values()):\n",
    "                verboseprint(\"Same markers were found to have duplicates, keeping only first occurrence\")\n",
    "                names_counts = collections.Counter(new_names.values()).most_common()\n",
    "                for (marker, count) in names_counts:\n",
    "                    if count > 1:\n",
    "                        verboseprint(\"dropping duplicates of\", marker)\n",
    "                        for column in [key for key, value in new_names.items() if value == marker][1:]:\n",
    "                            original_db.drop(inplace=True, columns=column)\n",
    "                            verboseprint(column, \"dropped\")\n",
    "\n",
    "            # renames columns to just marker name\n",
    "            original_db.rename(columns=new_names, inplace=True)  \n",
    "\n",
    "            # separate into db for markers and pos for spatial position\n",
    "            db = original_db.iloc[:, :-3]\n",
    "            pos = original_db.iloc[:, -3:-1]\n",
    "            print(\"data loaded\")\n",
    "\n",
    "\n",
    "        # discard unwanted markers, due to different possible reasons\n",
    "        if perform_features_selection:\n",
    "            db = features_selection(db, reference)\n",
    "\n",
    "            # measure the number of the remaining markers\n",
    "            n_markers = len(db.columns)\n",
    "\n",
    "        # # perform Lognormal Shrinkage (LNS) from Dall'Olio et al. (2023) :https://doi.org/10.3390/e25020354\n",
    "        # if perform_lognormal_shrinkage:\n",
    "        #     db = lognormal_shrinkage(db, subsampling=4, max_n_gaussians=max_n_gaussians, \n",
    "        #                              contraction_factor=contraction_factor, populations_plot=True)\n",
    "\n",
    "        else:\n",
    "            verboseprint(\"Not performing data transformation.\")\n",
    "\n",
    "\n",
    "\n",
    "        # =============================================================================\n",
    "        # UMAP EMBEDDING\n",
    "        # =============================================================================\n",
    "\n",
    "\n",
    "        if load_embed:\n",
    "            print(\"Loading UMAP embedding...\")\n",
    "            # loading pre-made embedding file (if available)\n",
    "            embedding = np.load(embedding_storage, allow_pickle=True)\n",
    "\n",
    "        else:\n",
    "            embedding = embed_dataset(db.iloc[:, :n_markers], save_embed=True, nn=nn, metric=metric)\n",
    "\n",
    "else:\n",
    "    # load only the nececessary information for clustering: i.e., umap's embedding coordinates, and the rest later\n",
    "    print(\"Loading UMAP embedding...\")\n",
    "    embedding = np.load(embedding_storage, allow_pickle=True)\n",
    "\n",
    "    \n",
    "    # perform HDBSCAN clustering on c\n",
    "    mindim = max(int(len(db)*0.00005), 10)  # minimum number of cells allowed to form a speparate cluster [0.05%,0.2%]\n",
    "    \n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=mindim, min_samples=mindim,\n",
    "                                cluster_selection_epsilon=0.1,  # between 0.08 and 0.12 usually\n",
    "                                cluster_selection_method='eom')\n",
    "    placeholder = clusterer.fit_predict(embedding)\n",
    "    verboseprint('clustering large db')\n",
    "    verboseprint(datetime.now())\n",
    "\n",
    "\n",
    "\n",
    "    # only now load the whole db\n",
    "\n",
    "    # load reference file for columns: define subgroups, discarded markers, etc.\n",
    "    reference = pd.read_csv(reference_file)\n",
    "    print(\"loaded reference file\")\n",
    "\n",
    "\n",
    "    # creating directories to store plots/embedding\n",
    "    for directory in [folder, folder+\"positions/\", folder+'expressions/', folder+\"markers/\"]:\n",
    "        if not os.path.isdir(directory):\n",
    "            os.makedirs(directory)\n",
    "            print(\"creating\", directory)\n",
    "        else:\n",
    "            verboseprint(\"using pre existing\", directory)\n",
    "\n",
    "    # load previously preprocessed db\n",
    "    db = pd.read_csv(base_folder+\"quantized_dbs/\"+area+\"_quantized(\"+str(max_n_gaussians)+\",\"+str(contraction_factor)+\").csv\")\n",
    "\n",
    "    # robust standardization\n",
    "    db = (db-db.median())/(np.abs(db - db.mean()).mean())\n",
    "\n",
    "    # select a specific marker subgroup, if required\n",
    "    if markers_subgroup:\n",
    "        for column in db.columns.values:\n",
    "            verboseprint(column)\n",
    "            marker = column.split(sep='_')[-1]\n",
    "\n",
    "            # <SPECIFIC FOR THE PAPER>\n",
    "            if marker == 'lambda':\n",
    "                ab = [a for a in reference.loc[:, correspondence_column] if marker in a][1]  # since kappasinelambda comes at 0\n",
    "            else:\n",
    "                ab = [a for a in reference.loc[:, correspondence_column] if marker in a][0]\n",
    "\n",
    "            # discard markers not in the specific subgroup\n",
    "            to_discard = reference[reference.loc[:, correspondence_column] == ab].loc[: , markers_subgroup].iloc[0] == 0\n",
    "            if to_discard:\n",
    "                db.drop(columns=column, inplace=True)\n",
    "                verboseprint(\"Dropping\", column, \"since this marker is not in\", markers_subgroup, 'subgroup.')\n",
    "\n",
    "    # measure the number of the remaining markers\n",
    "    n_markers = len(db.columns)\n",
    "\n",
    "    # assign clustering labels to db\n",
    "    db['cluster'] = placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d8ac3e-4f84-46ed-98b2-3a6cc8a71520",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_markers_on_embedding(db, original_db, embedding, pos, save=True, path=folder+\"markers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1593a0-ad7c-4683-847d-d3b1767fdce6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HDBSCAN clustering on UMAP embedding\n",
    "# =============================================================================\n",
    "\n",
    "if not large_db_procedure:\n",
    "    if load_clusters:\n",
    "        db['cluster'] = pd.read_csv(folder+area+\"__clusters_labels.csv\").loc[:, 'cluster']\n",
    "\n",
    "    else:\n",
    "        mindim = max(int(len(db)*0.00005), 10)  # minimum number of cells allowed to form a speparate cluster [0.05%,0.2%]\n",
    "\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=mindim, min_samples=mindim,\n",
    "                                    cluster_selection_epsilon=0.1,  # between 0.08 and 0.12 usually\n",
    "                                    cluster_selection_method='eom')\n",
    "        db['cluster'] = clusterer.fit_predict(embedding)\n",
    "\n",
    "\n",
    "plot_embedding_clusters(db['cluster'], pos, embedding, clusterer, save_plot=True, path=folder+area+\"__clusters.png\")\n",
    "    \n",
    "verboseprint(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cc882f-0cde-482c-b8c9-79da9d60ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional reclustering procedure <Specific for the paper>\n",
    "# tune another clustering with indipendent parameters on the biggest cluster\n",
    "# (which probably needs to be furtherly splitted)\n",
    "\n",
    "if not load_clusters:\n",
    "    \n",
    "    \n",
    "    # Find biggest cluster as candidate for the reclustering procedure\n",
    "    sizes = db.groupby('cluster').size()\n",
    "    if sizes.max() > len(db)*0.1:\n",
    "        sub_db = db[db['cluster'] == sizes.argmax()-1].copy()\n",
    "        sub_pos = pos[db['cluster'] == sizes.argmax()-1].copy()\n",
    "        candidate_reclustering_core = sizes.argmax()-1\n",
    "        \n",
    "    else:\n",
    "        candidate_reclustering_core = -1\n",
    "\n",
    "        \n",
    "    # evaluate if biggest cluster is candidate for further splitting,\n",
    "    # and in that case perform a reclustering of just the biggest cluster\n",
    "    if candidate_reclustering_core != -1 and sizes.max() > len(db)/10:\n",
    "        mask = db['cluster'] == candidate_reclustering_core\n",
    "        sub_db = db[mask].copy()\n",
    "        sub_embedding = embedding[mask].copy()\n",
    "        sub_pos = pos[mask].copy()\n",
    "\n",
    "        sub_clusterer = hdbscan.HDBSCAN(min_cluster_size=mindim*10, min_samples=mindim*10,\n",
    "                                        cluster_selection_epsilon=0.08,  # 0.08 for whole Lymph node, L1, CodeX\n",
    "                                                                         # 0.11 for L2, L3, T1, T2\n",
    "                                                                         # 0.15 for T3\n",
    "                                        cluster_selection_method='leaf')\n",
    "        sub_db['cluster'] = sub_clusterer.fit_predict(sub_embedding)\n",
    "\n",
    "        plot_embedding_clusters(sub_db['cluster'], sub_pos, sub_embedding, sub_clusterer, save_plot=True, \n",
    "                                path=folder+area+\"__clusters_reclustering_core_zoom.png\")\n",
    "\n",
    "\n",
    "\n",
    "        verboseprint(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026210f8-bc88-4dbe-97ad-081658a5a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if optional reclustering procedure has been performed, \n",
    "# overwrite biggest cluster labels with new ones <Specific for the paper>\n",
    "\n",
    "if not load_clusters:\n",
    "\n",
    "    if candidate_reclustering_core != -1 and sizes.max() > len(db)/10:\n",
    "\n",
    "        sub_db['cluster'] = np.where(sub_db['cluster'] < 1, sub_db['cluster'], sub_db['cluster']+max(db['cluster']))\n",
    "        sub_db['cluster'] = np.where(sub_db['cluster'] == 0, candidate_reclustering_core, sub_db['cluster'])\n",
    "\n",
    "        db.loc[sub_db.index, 'cluster'] = sub_db['cluster']\n",
    "\n",
    "        plot_embedding_clusters(db['cluster'], pos, embedding, clusterer, save_plot=True, \n",
    "                                path=folder+area+\"__clusters.png\")\n",
    "\n",
    "        verboseprint(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292730f6-1cef-46aa-a199-1f4f8eab0171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ASSIGN CLUSTER SIGNIFICANCE\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# choose on which db to calculate the metrics\n",
    "select = \"original\"  # \"original\" suggested\n",
    "\n",
    "if select.lower() == 'original':\n",
    "    use = original_db.loc[:, db.columns[:n_markers]]  # use original db and\n",
    "    use = (use-use.median())/(np.abs(use - use.mean()).mean())             # robustly standardize it\n",
    "else:\n",
    "    use = db.iloc[:, :n_markers]                      # use modified db\n",
    "\n",
    "p_val_basic_threshold = 0.001  # p-value for a single test\n",
    "\n",
    "# thresholds for pvalues (statistical significance) and for effect sizes (difference magnitude)\n",
    "# using Bonferroni correction, most stringent one w.r.t. false positivies (e.g. falsely expressed markers)\n",
    "thrs_p = p_val_basic_threshold/len(db.columns[:-1])/len(np.unique(db.loc[:, 'cluster']))\n",
    "    \n",
    "    \n",
    "# compute each markers effect size and p-value for every cluster    \n",
    "res, res_p = markers_importance_per_cluster(use, db.loc[:, 'cluster'], compare_with='rest')\n",
    "\n",
    "\n",
    "# update db with MainMarkers and interpretative column for top 'find_n'\n",
    "# most relevant effect sizes\n",
    "db = add_main_markers_significance_columns(db, res, thrs_p=thrs_p)\n",
    "\n",
    "\n",
    "# plot the 2 added columns for every cluster over the whole db\n",
    "whole_dataset_summary_plots(db, area, save_plot=True, base_path=folder)\n",
    "\n",
    "\n",
    "\n",
    "# Find and order important markers (in our specific case, lineage defining markers)\n",
    "# which will be used to monitor the nature of each cluster and compare it to the whole sample\n",
    "monitoring_features = []\n",
    "\n",
    "# <Specific for the paper>\n",
    "if area == \"DatasetCodex\":\n",
    "    monitoring_features = use.columns\n",
    "    \n",
    "else:\n",
    "    for column in use.columns:\n",
    "        \n",
    "        # find which features are part of the monitoring group and store their names \n",
    "        # in 'monitoring_features'\n",
    "        try:\n",
    "            ab = [a for a in reference.loc[:, correspondence_column] if column in a][0]\n",
    "            if reference[reference.loc[:, correspondence_column] == ab].loc[:, importance_column].iloc[0]:\n",
    "                monitoring_features.append(column)\n",
    "        except IndexError:\n",
    "            verboseprint(column, \"not present in reference file. Cannot establish if it is a monitoring feature\")\n",
    "            continue\n",
    "\n",
    "\n",
    "# order markers alphabetically to always have the same set across different samples\n",
    "monitoring_features = np.asarray(monitoring_features)[np.argsort(monitoring_features)]\n",
    "\n",
    "\n",
    "verboseprint(\"clusters:\", np.unique(db.loc[:, 'cluster']))\n",
    "\n",
    "\n",
    "for i in np.unique(db.loc[:, 'cluster']):\n",
    "    verboseprint(\"producing results for cluster\", i)\n",
    "\n",
    "    # select cells within cluster i-th\n",
    "    mask = db.loc[:, 'cluster'] == i  \n",
    "\n",
    "    # select res and res_p corresponding to cluster i-th\n",
    "    rs = np.asarray(res.loc[i, :])\n",
    "    rs_p = np.asarray(res_p.loc[i, :])\n",
    "\n",
    "    # <Specific for the paper>\n",
    "    if area == 'DatasetCodex':\n",
    "        n = np.min([14, len(rs)])  # consider a fixed maximum number of markers\n",
    "    else:\n",
    "        n = np.min([30, len(rs)])  # consider a fixed maximum number of markers\n",
    "\n",
    "    # making effect sizes plot (ranking markers in descending order of effect size)\n",
    "    # and building the 'lbl' variable, a string with Tier 1 (probably important markers)\n",
    "    # and Tier 2 (possibly important markers)\n",
    "    lbl = most_important_effect_sizes(rs, rs_p, n, thrs_p=thrs_p, save_plot=True, path=folder+\"expressions/\"+area+\"__cluster_\"+str(i)+\"_expression.png\")\n",
    "\n",
    "\n",
    "    # making postion plot (where are cluster cells) and reporting most expressed markers\n",
    "    plot_cluster_spatial_location(lbl, embedding, pos, mask, save_plot=True, path=folder+\"positions/\"+area+\"__cluster_\"+str(i)+\"_position.png\")\n",
    "\n",
    "\n",
    "    # Monitoring features plot\n",
    "    monitoring_features_plot(original_db, mask=mask, cluster_name='cluster '+str(i), monitoring_features=monitoring_features, save_plot=True, path=folder+\"expressions/\"+area+\"__cluster_\"+str(i)+\"_monitoring_featuresining.png\")\n",
    "    \n",
    "    \n",
    "verboseprint(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0e00d2-f52a-4f4d-bca9-982548c27006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "if not load_clusters:\n",
    "    # cluster labels\n",
    "    db.loc[:, \"cluster\"].to_csv(folder+area+\"__clusters_labels.csv\")\n",
    "\n",
    "    # cluster sizes\n",
    "    sizes = db.groupby('cluster').size()\n",
    "    sizes.to_excel(folder+area+\"__clusters_sizes.xlsx\", index_label='cluster')\n",
    "    verboseprint(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce0dcb-ac85-43c5-970d-b12a942da694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset plot params to default\n",
    "plt.rcParams.update({'font.size': old_fontsize})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
